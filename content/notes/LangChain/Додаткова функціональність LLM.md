

імпорт CodeBlock з «@theme/CodeBlock»; імпорт Example з «@examples/models/llm/llm.ts»; імпорт DebuggingExample з «@examples/models/llm/llm_debugging.ts»; імпорт StreamingExample з «@examples/models/llm/llm_streaming.ts»; імпорт TimeoutExample з «@examples/models/llm/llm_timeout.ts»; імпорт CancellationExample з «@examples/models/llm/llm_cancellation.ts»;

# Додаткова функціональність: LLM

Ми пропонуємо ряд додаткових функцій для LLM. У більшості наведених нижче прикладів ми використовуватимемо `OpenAI` ТЛМ. Однак всі ці функції доступні для всіх LLM.

## Додаткові методи

LangChain надає ряд додаткових методів для взаємодії з LLM:

<CodeBlock language="typescript">{Приклад}</CodeBlock>

## Потокові відповіді

Деякі LLM-модулі надають функцію потокового передавання. Це означає, що замість того, щоб чекати на повернення всієї відповіді, ви можете розпочати її обробку, як тільки вона стане доступною. Це корисно, якщо ви хочете, щоб відображалася відповідь користувачу в процесі її створення, або якщо ви хочете обробити відповідь так, як вона генерується. LangChain в даний час забезпечує потокове передавання для `OpenAI` LLM:

<CodeBlock language="typescript">{StreamingПриклад}</CodeBlock>

## Кешування

LangChain надає додатковий рівень кешування для LLM. Це корисно з двох причин:

1. Це може заощадити ваші гроші, зменшуючи кількість викликів API до постачальника LLM, якщо ви часто запитуєте одне і те ж завершення кілька разів.
2. Це може прискорити вашу програму, зменшуючи кількість викликів API, які ви робите постачальнику LLM.

### Кешування в пам'яті

Кеш за замовчуванням зберігається в пам'яті. Це означає, що якщо ви перезапустите програму, кеш буде очищений.

Для того, щоб включити його ви можете пройти `cache: true` коли ви інстанціюєте LLM. Наприклад:


```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ cache: true });
```

### Кешування з Redis

LangChain також надає кеш на базі Redis. Це корисно, якщо ви хочете поділитися кешем між декількома процесами або серверами. Щоб використовувати його, вам потрібно встановити `redis` пакунок:


```bash npm2yarn
npm install redis
```

Потім, ви можете пройти `cache` варіант, коли ви вставляєте LLM. Наприклад:


```typescript
import { OpenAI } from "langchain/llms/openai";
import { RedisCache } from "langchain/cache/redis";
import { createClient } from "redis";

// See https://github.com/redis/node-redis for connection options
const client = createClient();
const cache = new RedisCache(client);

const model = new OpenAI({ cache });
```

## Додавання часу очікування

За замовчуванням, LangChain буде чекати невизначений час для відповіді від постачальника моделі. Якщо ви хочете додати тайм-аут, ви можете передати `timeout` параметр, у мілісекундах, коли ви викликаєте модель. Наприклад, для OpenAI:

<CodeBlock language="typescript">{ExampleTimeout}</CodeBlock>

## Скасування запитів

Ви можете скасувати запит, пройшовши `signal` варіант, коли ви викликаєте модель. Наприклад, для OpenAI:

<CodeBlock language="typescript">{Приклад_скасування}</CodeBlock>

Зауважте, що вихідний запит буде скасовано, лише якщо постачальник, який його надав, надасть цю можливість. LangChain скасує базовий запит, якщо це можливо, в іншому випадку він скасує обробку відповіді.

## Робота з лімітами ставок

Деякі LLM-провайдери мають ліміти ставок. Якщо ви перевищите ліміт тарифу, то отримаєте повідомлення про помилку. Щоб допомогти вам впоратися з цим, LangChain надає `maxConcurrency` варіант під час створення екземпляра LLM. За допомогою цього параметра ви можете вказати максимальну кількість одночасних запитів, які слід зробити постачальнику LLM. Якщо ви перевищите цей номер, LangChain автоматично поставить в чергу ваші запити, які будуть відправлені після завершення попередніх запитів.

Наприклад, якщо ви встановили `maxConcurrency: 5`, Тоді LangChain буде відправляти тільки 5 запитів до постачальника LLM в той час. Якщо ви надішлете 10 запитів, перші 5 будуть відправлені негайно, а наступні 5 будуть поставлені в чергу. Після того, як один з перших 5 запитів завершиться, наступний запит в черзі буде відправлений.

Щоб використовувати цю функцію, просто перейдіть `maxConcurrency: <number>` коли ви інстанціюєте LLM. Наприклад:


```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ maxConcurrency: 5 });
```

## Робота з помилками API

Якщо постачальник моделі повертає помилку від свого API, за замовчуванням LangChain намагатиметься до 6 разів на експоненційному фоні. Це дозволяє відновити помилку без будь-яких додаткових зусиль від вас. Якщо ви хочете змінити цю поведінку, ви можете передати `maxRetries` варіант при інстанціюванні моделі. Наприклад:


```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ maxRetries: 10 });
```

## Підписка на події

Особливо при використанні агента, може бути багато вперед-назад відбувається за лаштунками, як LLM обробляє підказку. Для агентів об'єкт відповіді містить об'єкт intermediateSteps, який можна надрукувати, щоб переглянути огляд кроків, які він взяв, щоб потрапити туди. Якщо цього недостатньо і ви хочете бачити кожен обмін з LLM, ви можете передати зворотні виклики до LLM для користувацького журналювання (або що-небудь ще, що ви хочете зробити), як модель проходить через кроки:

Щоб дізнатися більше про доступні події, див. [Callbacks](/docs/production/callbacks/) розділ документації.

<CodeBlock language="typescript">{DebuggingExample}</CodeBlock>
